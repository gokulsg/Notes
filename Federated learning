Federated learning is a decentralized collaborative machine learning approach.

The term Federated Learning was coined by Google in a paper first published in 2016. Since then, it has been an area of active research as evidenced by papers published on arXiv. In the recent TensorFlow Dev Summit, Google unveiled TensorFlow Federated (TFF), making it more accessible to users of its popular deep learning framework. 

Meanwhile, for PyTorch users, the open-source community OpenMined had already made available the PySyft library since the end of last year with a similar goal.

Standard machine learning approaches require centralizing the training data on one machine or in a datacenter. 
And Google has built one of the most secure and robust cloud infrastructures for processing this data to make our services better. 
Now for models trained from user interaction with mobile devices, we're introducing an additional approach: Federated Learning.
Federated Learning enables mobile phones to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud. 
This goes beyond the use of local models that make predictions on mobile devices (like the Mobile Vision API and On-Device Smart Reply) by bringing model training to the device as well.
It works like this: your device downloads the current model, improves it by learning from data on your phone, and then summarizes the changes as a small focused update. 
Only this update to the model is sent to the cloud, using encrypted communication, where it is immediately averaged with other user updates to improve the shared model. All the training data remains on your device, and no individual updates are stored in the cloud. 

Federated Learning allows for smarter models, lower latency, and less power consumption, all while ensuring privacy.
And this approach has another immediate benefit: in addition to providing an update to the shared model, the improved model on your phone can also be used immediately, powering experiences personalized by the way you use your phone.

To make Federated Learning possible, google had to overcome many algorithmic and technical challenges. 
In a typical machine learning system, an optimization algorithm like Stochastic Gradient Descent (SGD) runs on a large dataset partitioned homogeneously across servers in the cloud. 
Such highly iterative algorithms require low-latency, high-throughput connections to the training data. 
But in the Federated Learning setting, the data is distributed across millions of devices in a highly uneven fashion. 
In addition, these devices have significantly higher-latency, lower-throughput connections and are only intermittently available for training.
These bandwidth and latency limitations motivate our Federated Averaging algorithm, which can train deep networks using 10-100x less communication compared to a naively federated version of SGD. 
The key idea is to use the powerful processors in modern mobile devices to compute higher quality updates than simple gradient steps. 
Since it takes fewer iterations of high-quality updates to produce a good model, training can use much less communication. 
As upload speeds are typically much slower than download speeds, we also developed a novel way to reduce upload communication costs up to another 100x by compressing updates using random rotations and quantization. 
While these approaches are focused on training deep networks, google also designed algorithms for high-dimensional sparse convex models which excel on problems like click-through-rate prediction.

Google had successfully included Federated learning on GBoard.
